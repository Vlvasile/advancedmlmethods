{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa70d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gradio\n",
    "import random\n",
    "import gradio as gr\n",
    "import json \n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# open the data file we have\n",
    "# if you run the code in Colab, match the path in to your Google Drive\n",
    "with open('aurabot.json') as file:\n",
    "    # data = json.load(file)\n",
    "    file_data = file.read()\n",
    "    \n",
    "# MODIFY THE KEY VALUES\n",
    "file_data = file_data.replace('\"question\"', '\"patterns\"')\n",
    "file_data = file_data.replace('\"answer\"', '\"responses\"')\n",
    "\n",
    "# now load the processed text-JSON into Python dictionary\n",
    "data = json.loads(file_data)\n",
    "\n",
    "# load the model\n",
    "model = keras.models.load_model('aurabot_chat_model.keras')\n",
    "\n",
    "# load tokenizer object\n",
    "with open('aurabot_chat_tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# load label encoder object\n",
    "with open('aurabot_chat_label_encoder.pickle', 'rb') as enc:\n",
    "    lbl_encoder = pickle.load(enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b969b090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected: machine\n"
     ]
    }
   ],
   "source": [
    "# pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "word = \"maechine\"\n",
    "corrected = TextBlob(word).correct()\n",
    "corrected = str(corrected)\n",
    "print(\"Corrected: \" + corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e154545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error\n",
    "from urllib.parse import quote\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "\n",
    "# create function for downloading a word definition\n",
    "def get_word_definition(word):\n",
    "    # this function uses this API:\n",
    "    # https://dictionaryapi.dev/\n",
    "\n",
    "\n",
    "    # EXTRA: use TextBlob module to fix a typo if the user\n",
    "    # didn't write the defined word correctly\n",
    "    corrected = TextBlob(word).correct()\n",
    "    corrected = str(corrected)\n",
    "    prefix = \"\"\n",
    "\n",
    "    # fix the typo if present and inform user it was fixed\n",
    "    if corrected != word:\n",
    "        word = corrected\n",
    "        prefix = \"Small typo fixed: \" + word + \"!\\n\"\n",
    "\n",
    "    # if \"word\" has a space, the URL will be broken\n",
    "    # we have to replace the spaces in the URL with HTML entities\n",
    "    word = quote(word)\n",
    "\n",
    "    # let's quickly test our API works\n",
    "    url = \"https://api.dictionaryapi.dev/api/v2/entries/en/\" + word\n",
    "\n",
    "    # many modern URLs / APIs require a valid header including the user agent\n",
    "    # basically we mimic a web browser here\n",
    "    # Define headers, including a User-Agent (this was asked from ChatGPT)\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # attempt downloading the JSON response from the URL\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        raw_data = urllib.request.urlopen(req).read().decode(\"UTF-8\")\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(e)\n",
    "        return \"Sorry, I couldn't find a definition for that word.\"\n",
    "    except urllib.error.URLError as e:\n",
    "        print(e)\n",
    "        return \"Sorry, I couldn't find a definition for that word.\"\n",
    "    else:\n",
    "        data = json.loads(raw_data)\n",
    "        \n",
    "        # with the headers, the data is correctly downloaded!\n",
    "        # now we have to extract the definition from the data\n",
    "\n",
    "        # based on the data:\n",
    "        # list -> dictionary[0] => meanings[0] => definitions[0] => definition\n",
    "        filtered_data = data[0]['meanings'][0]['definitions'][0]['definition']\n",
    "        return prefix + \"Definition: \" + filtered_data\n",
    "    \n",
    "\n",
    "# if we try to search \"copmuter\" which is a typo in \"computer\"\n",
    "# the API doesn't work. However, there's a module in Python called TextBlob\n",
    "# that allows us to detect and fix common typos\n",
    "\n",
    "# you can test the function like this:\n",
    "# get_word_definition(\"machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96faf5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    }
   ],
   "source": [
    "# max len value as the same as in training phase\n",
    "max_len = 30\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# the chat-function connected to UI\n",
    "def chat_response(message, history):\n",
    "    # we have to connect the variables outside the function in order to use this\n",
    "    global model, tokenizer, lbl_encoder, max_len\n",
    "    \n",
    "    # input is the message-parameter from gradio\n",
    "    \n",
    "    # собираем контекст из последних 3 сообщений + текущее сообщение\n",
    "    context = \" \".join([msg[\"content\"] for msg in history[-3:]]) + \" \" + message\n",
    "    \n",
    "    # токенизируем весь контекст, а не только текущее сообщение\n",
    "    sequence = tokenizer.texts_to_sequences([context])\n",
    "    padded = keras.preprocessing.sequence.pad_sequences(sequence, truncating='post', maxlen=max_len)\n",
    "\n",
    "    # if user wants a definition, use the helper function to fetch an answer\n",
    "    # the function is able to fix small typos in the user's given word!\n",
    "    if \"define:\" in context:\n",
    "        parts = context.split(\"define:\")\n",
    "        inp = parts[1].strip()\n",
    "        definition = get_word_definition(inp)\n",
    "        return definition\n",
    "    else:\n",
    "        # run the input through the model and process the result\n",
    "        result = model.predict(padded)\n",
    "        \n",
    "        # can somehow get the confidence from the model regarding this result?\n",
    "        confidence = round(np.amax(result) * 100, 1)\n",
    "\n",
    "        # a good cutoff point is probably somewhere between 70-80%\n",
    "        # because anything under 90% seems to be the wrong answer\n",
    "        # the confidence % is also added into the bot's messages\n",
    "        # so we can test our chatbot's answer quality better\n",
    "        if confidence < 80:\n",
    "            return \"Sorry, I don't know the answer\" + f\" ({confidence} %)\"\n",
    "\n",
    "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "\n",
    "        # find the correct set of responses, and return a random one\n",
    "        for i in data['intents']:\n",
    "            if i['tag'] == tag:\n",
    "                current_response = np.random.choice(i['responses'])\n",
    "\n",
    "                # replace time placeholder in aurabot data with the actual timestamp\n",
    "                # you can always prettify the timestamp into some better format\n",
    "                # like: 26.3.2025 10:45:35\n",
    "                if \"{CURRENT_TIME}\" in current_response:\n",
    "                    current_response = current_response.replace(\"{CURRENT_TIME}\", str(datetime.now()))\n",
    "\n",
    "                return current_response + f\" ({confidence} %)\"\n",
    "\n",
    "\n",
    "# launch chat UI\n",
    "demo = gr.ChatInterface(chat_response, type=\"messages\", autofocus=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
